{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syedalimuradtahir/Documents/Personal Projects/Portfolio_new/portfolio_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from backend.Modules.Services import *\n",
    "from backend.Modules.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdata_extracted = extract_text_from_json('backend/personal_data/work_experience.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Company: Tesla, Role: Data Engineer. During my tenure at Tesla as a Data Engineer, I spearheaded the development of Grafana dashboards to monitor real-time production metrics, delivering high-quality insights to over 100 stakeholders. I designed and implemented robust ETL pipelines in Airflow with Python and SQL to ensure data quality and reliability across key metrics, and created visual dashboards to track KPIs such as machine performance and uptime—providing actionable insights for effective decision-making. I also implemented comprehensive data visualizations that tracked machine states, enabling management to swiftly address downtime root causes and enhance productivity. In addition, I built real-time data ingestion pipelines that integrated sensor and machine data from PLCs into centralized data warehouses and marts, developed YAML files for CI/CD automation to ensure consistent code integration, and deployed applications and data pipelines in Kubernetes clusters using Docker for enhanced scalability and manageability. Furthermore, I created personalized dashboard plugins that allowed users to input reasons for machine states, leveraging Pareto and pie charts to analyze downtime, and collaborated with software engineering teams to optimize data pipelines using PySpark for large-scale distributed processing. I also optimized the backend of a Grafana plugin using Golang and improved its frontend using TypeScript and React, resulting in significant improvements in processing speed and overall efficiency.',\n",
       " 'Company: GetSteps, Role: Business Intelligence Analyst. At GetSteps, I designed and implemented robust data pipelines using PostgreSQL and dbt, significantly improving data availability and streamlining ETL workflows. I developed custom dashboards and KPI comparison reports in Metabase that provided critical insights into marketing performance and customer engagement. By enhancing data marts with additional KPIs, I enabled detailed cohort analysis for the marketing team, while also establishing a new data model and reorganizing the data marts to improve governance and transparency of key metrics. I conducted custom cohort analyses using Jupyter Notebooks to examine Customer Lifetime Value (CLV) and created a comprehensive shopping funnel report with KPIs and visualizations. Additionally, I generated overview reports for outstanding orders involving discontinued cover materials to assist the production team in inventory management, performed ad-hoc analyses for the CEO—including segmented sales performance reports for B2B and B2B2C—and leveraged Google Analytics and Airbyte to import data from multiple sources, thereby enhancing overall data comprehensiveness.',\n",
       " 'Company: Europcar Mobility Group, Role: Business Intelligence Analyst. At Europcar Mobility Group, I designed reports using SAP Business Objects (BO) and Google Data Studio to provide actionable insights that supported strategic decision-making. I implemented ETL pipelines in BigQuery using SQL to extract and integrate data from SAP BO and Oracle, ensuring high data accuracy and consistency. Additionally, I built dynamic dashboards in Looker to empower stakeholders across departments to track key performance metrics effectively. I automated reporting processes using VBA macros in Excel and Access, significantly reducing manual effort, and managed the scheduling of automated report generation in SAP BO and virtual machines. I also developed detailed documentation for data marts and dashboards to support data governance initiatives, maintained and updated the company’s data model across multiple sources, and conducted training sessions for business users to enhance their ability to utilize these analytical tools.',\n",
       " 'Company: Rohde & Schwarz, Role: BI Software Developer. At Rohde & Schwarz, I developed robust time series forecasting models in Tableau and Python to accurately predict sales trends and optimize inventory management. I created interactive dashboards in Tableau that enabled stakeholders to analyze sales performance and derive actionable insights. Additionally, I presented data-driven visualizations to support informed decision-making and designed as well as maintained comprehensive ETL workflows using SQL to ensure consistent and reliable data integration across the organization.',\n",
       " 'Project: Aircraft Design Improvement using ML and Gen AI. In this project, I utilized machine learning and deep learning techniques—including Random Forest, KNN, CNN, and Generative AI models—to predict key aircraft parameters such as range and flight mechanics. The project involved extensive data preprocessing, model training, and evaluation to ensure that the predictive models could accurately inform improvements in aircraft design.',\n",
       " 'Project: Enhancing Movie Recommendations with Behavior Sequence Transformer. I developed a Transformer-based behavior sequence model for a movie recommendation system, training it on a dataset of 5 million entries. The project involved comparing its performance with traditional recommendation methods, ultimately enhancing user personalization by leveraging TensorFlow and Keras.',\n",
       " 'Project: Financial News Sentiment Analysis. This project involved classifying financial news articles into positive, neutral, or negative sentiments using a combination of machine learning and deep learning techniques. I utilized tools such as NLTK, TensorFlow, Keras, and Scikit-learn to preprocess data, train models, and achieve accurate sentiment analysis for informed decision-making.',\n",
       " 'Project: Stock Price Prediction using LSTM & Fbprophet. I implemented time series forecasting models using LSTM, ARIMA, and Facebook Prophet to predict stock prices. The project included data preparation, model development, and performance comparison, ultimately identifying the most effective forecasting approach.',\n",
       " 'Project: Customer Segmentation using Deep Learning. In this project, I developed a deep learning-based customer segmentation model that achieved 98% accuracy. This model was used to categorize customers for targeted marketing campaigns, thereby optimizing marketing strategies and enhancing customer engagement.',\n",
       " 'Project: Extracting & Analyzing Contract Information Using LLMs. I utilized open-source large language models (LLMs), such as LLAMA 2.0 and Mixtral, to extract and analyze contract information from legal documents. This project automated the analysis workflow and provided valuable insights into contract law.',\n",
       " 'Project: Large Language Models for Reliable Berlin Public Administration Answers. I developed and deployed a chatbot system for Berlin’s public administration that leveraged various open-source LLMs (including ChatGPT 3.5, LLAMA, Alpaca, Vicuna 13b, Dolly 2.0, and Raven) to provide reliable, context-based answers to public queries.',\n",
       " 'Project: Designing Seq2Seq Translation Model with Attention Mechanism. I created a Seq2Seq translation model with an attention mechanism to translate between English and Bulgarian. The project involved using GloVe and fastText embeddings for word representation and training the model using modern deep learning frameworks.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workdata_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It is a good idea to use a small chunk size for tasks that require a fine-grained view of the text and a larger chunk size \n",
    "for tasks that require a more holistic view of the text.\n",
    "\n",
    "Since we want a more Holistic view as it's better for chatbot Q/A: https://dev.to/peterabel/what-chunk-size-and-chunk-overlap-should-you-use-4338\n",
    "\"\"\"\n",
    "\n",
    "work_data = split_text(workdata_extracted, chunk_size=500, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Company: Tesla, Role: Data Engineer. During my tenure at Tesla as a Data Engineer, I spearheaded the development of Grafana dashboards to monitor real-time production metrics, delivering high-quality insights to over 100 stakeholders. I designed and implemented robust ETL pipelines in Airflow with Python and SQL to ensure data quality and reliability across key metrics, and created visual dashboards to track KPIs such as machine performance and uptime—providing actionable insights for effective'),\n",
       " Document(metadata={}, page_content='for effective decision-making. I also implemented comprehensive data visualizations that tracked machine states, enabling management to swiftly address downtime root causes and enhance productivity. In addition, I built real-time data ingestion pipelines that integrated sensor and machine data from PLCs into centralized data warehouses and marts, developed YAML files for CI/CD automation to ensure consistent code integration, and deployed applications and data pipelines in Kubernetes clusters'),\n",
       " Document(metadata={}, page_content='Kubernetes clusters using Docker for enhanced scalability and manageability. Furthermore, I created personalized dashboard plugins that allowed users to input reasons for machine states, leveraging Pareto and pie charts to analyze downtime, and collaborated with software engineering teams to optimize data pipelines using PySpark for large-scale distributed processing. I also optimized the backend of a Grafana plugin using Golang and improved its frontend using TypeScript and React, resulting in'),\n",
       " Document(metadata={}, page_content='React, resulting in significant improvements in processing speed and overall efficiency.'),\n",
       " Document(metadata={}, page_content='Company: GetSteps, Role: Business Intelligence Analyst. At GetSteps, I designed and implemented robust data pipelines using PostgreSQL and dbt, significantly improving data availability and streamlining ETL workflows. I developed custom dashboards and KPI comparison reports in Metabase that provided critical insights into marketing performance and customer engagement. By enhancing data marts with additional KPIs, I enabled detailed cohort analysis for the marketing team, while also establishing'),\n",
       " Document(metadata={}, page_content='also establishing a new data model and reorganizing the data marts to improve governance and transparency of key metrics. I conducted custom cohort analyses using Jupyter Notebooks to examine Customer Lifetime Value (CLV) and created a comprehensive shopping funnel report with KPIs and visualizations. Additionally, I generated overview reports for outstanding orders involving discontinued cover materials to assist the production team in inventory management, performed ad-hoc analyses for the'),\n",
       " Document(metadata={}, page_content='analyses for the CEO—including segmented sales performance reports for B2B and B2B2C—and leveraged Google Analytics and Airbyte to import data from multiple sources, thereby enhancing overall data comprehensiveness.'),\n",
       " Document(metadata={}, page_content='Company: Europcar Mobility Group, Role: Business Intelligence Analyst. At Europcar Mobility Group, I designed reports using SAP Business Objects (BO) and Google Data Studio to provide actionable insights that supported strategic decision-making. I implemented ETL pipelines in BigQuery using SQL to extract and integrate data from SAP BO and Oracle, ensuring high data accuracy and consistency. Additionally, I built dynamic dashboards in Looker to empower stakeholders across departments to track'),\n",
       " Document(metadata={}, page_content='to track key performance metrics effectively. I automated reporting processes using VBA macros in Excel and Access, significantly reducing manual effort, and managed the scheduling of automated report generation in SAP BO and virtual machines. I also developed detailed documentation for data marts and dashboards to support data governance initiatives, maintained and updated the company’s data model across multiple sources, and conducted training sessions for business users to enhance their'),\n",
       " Document(metadata={}, page_content='to enhance their ability to utilize these analytical tools.'),\n",
       " Document(metadata={}, page_content='Company: Rohde & Schwarz, Role: BI Software Developer. At Rohde & Schwarz, I developed robust time series forecasting models in Tableau and Python to accurately predict sales trends and optimize inventory management. I created interactive dashboards in Tableau that enabled stakeholders to analyze sales performance and derive actionable insights. Additionally, I presented data-driven visualizations to support informed decision-making and designed as well as maintained comprehensive ETL workflows'),\n",
       " Document(metadata={}, page_content='ETL workflows using SQL to ensure consistent and reliable data integration across the organization.'),\n",
       " Document(metadata={}, page_content='Project: Aircraft Design Improvement using ML and Gen AI. In this project, I utilized machine learning and deep learning techniques—including Random Forest, KNN, CNN, and Generative AI models—to predict key aircraft parameters such as range and flight mechanics. The project involved extensive data preprocessing, model training, and evaluation to ensure that the predictive models could accurately inform improvements in aircraft design.'),\n",
       " Document(metadata={}, page_content='Project: Enhancing Movie Recommendations with Behavior Sequence Transformer. I developed a Transformer-based behavior sequence model for a movie recommendation system, training it on a dataset of 5 million entries. The project involved comparing its performance with traditional recommendation methods, ultimately enhancing user personalization by leveraging TensorFlow and Keras.'),\n",
       " Document(metadata={}, page_content='Project: Financial News Sentiment Analysis. This project involved classifying financial news articles into positive, neutral, or negative sentiments using a combination of machine learning and deep learning techniques. I utilized tools such as NLTK, TensorFlow, Keras, and Scikit-learn to preprocess data, train models, and achieve accurate sentiment analysis for informed decision-making.'),\n",
       " Document(metadata={}, page_content='Project: Stock Price Prediction using LSTM & Fbprophet. I implemented time series forecasting models using LSTM, ARIMA, and Facebook Prophet to predict stock prices. The project included data preparation, model development, and performance comparison, ultimately identifying the most effective forecasting approach.'),\n",
       " Document(metadata={}, page_content='Project: Customer Segmentation using Deep Learning. In this project, I developed a deep learning-based customer segmentation model that achieved 98% accuracy. This model was used to categorize customers for targeted marketing campaigns, thereby optimizing marketing strategies and enhancing customer engagement.'),\n",
       " Document(metadata={}, page_content='Project: Extracting & Analyzing Contract Information Using LLMs. I utilized open-source large language models (LLMs), such as LLAMA 2.0 and Mixtral, to extract and analyze contract information from legal documents. This project automated the analysis workflow and provided valuable insights into contract law.'),\n",
       " Document(metadata={}, page_content='Project: Large Language Models for Reliable Berlin Public Administration Answers. I developed and deployed a chatbot system for Berlin’s public administration that leveraged various open-source LLMs (including ChatGPT 3.5, LLAMA, Alpaca, Vicuna 13b, Dolly 2.0, and Raven) to provide reliable, context-based answers to public queries.'),\n",
       " Document(metadata={}, page_content='Project: Designing Seq2Seq Translation Model with Attention Mechanism. I created a Seq2Seq translation model with an attention mechanism to translate between English and Bulgarian. The project involved using GloVe and fastText embeddings for word representation and training the model using modern deep learning frameworks.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experience_store = store_embeddings(work_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syedalimuradtahir/Documents/Personal Projects/Portfolio_new/backend/Modules/Services/vector_store.py:83: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(\n"
     ]
    }
   ],
   "source": [
    "experience_store = store_openai_embeddings(work_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: Europcar Mobility Group, Role: Business Intelligence Analyst. At Europcar Mobility Group, I designed reports using SAP Business Objects (BO) and Google Data Studio to provide actionable insights that supported strategic decision-making. I implemented ETL pipelines in BigQuery using SQL to extract and integrate data from SAP BO and Oracle, ensuring high data accuracy and consistency. Additionally, I built dynamic dashboards in Looker to empower stakeholders across departments to track\n",
      "Company: Rohde & Schwarz, Role: BI Software Developer. At Rohde & Schwarz, I developed robust time series forecasting models in Tableau and Python to accurately predict sales trends and optimize inventory management. I created interactive dashboards in Tableau that enabled stakeholders to analyze sales performance and derive actionable insights. Additionally, I presented data-driven visualizations to support informed decision-making and designed as well as maintained comprehensive ETL workflows\n"
     ]
    }
   ],
   "source": [
    "query = \"What tasks did Burny do at Europcar Mobility Group?\"\n",
    "retrieved_docs = retrieve_text('backend/Modules/vector_store/chromadb', query)\n",
    "retrieved_text = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "print(retrieved_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syedalimuradtahir/Documents/Personal Projects/Portfolio_new/portfolio_env/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Europcar Mobility Group, Burny designed reports using SAP Business Objects (BO) and Google Data Studio to provide actionable insights that supported strategic decision-making. Burny also implemented ETL pipelines in BigQuery using SQL to extract and integrate data from SAP BO and Oracle, ensuring high data accuracy and consistency. Additionally, Burny built dynamic dashboards in Looker to empower stakeholders across departments to track key metrics.\n"
     ]
    }
   ],
   "source": [
    "llm_model = Chatbot(DEEPSEEK_MODEL)\n",
    "response = llm_model.generate_text(query, context=retrieved_text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
