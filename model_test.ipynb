{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.Modules.Services import *\n",
    "from backend.Modules.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdata_extracted = extract_text_from_json('backend/personal_data/work_experience.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_data = split_text(workdata_extracted, chunk_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Company: Tesla, Role: Data Engineer'),\n",
       " Document(metadata={}, page_content='Developed Grafana dashboards for monitoring real-time production metrics, delivering high-quality insights for 100+ stakeholders.'),\n",
       " Document(metadata={}, page_content='Created ETL pipelines in Airflow with Python and SQL, ensuring data quality and reliability across key metrics.'),\n",
       " Document(metadata={}, page_content='Designed visual dashboards to track KPI metrics, including machine performance and uptime, providing actionable insights for decision-making.'),\n",
       " Document(metadata={}, page_content='Implemented data visualizations to track machine states, allowing management to address downtime root causes and improve productivity.'),\n",
       " Document(metadata={}, page_content='Created engineering dashboards used by 100+ stakeholders across the USA, Germany, and Canada.'),\n",
       " Document(metadata={}, page_content='Created and presented data visualizations and analytical reports to senior management, providing actionable insights into machine performance and operational efficiency.'),\n",
       " Document(metadata={}, page_content='Built real-time data ingestion ETL pipelines, integrating sensor and machine data from PLCs into the data warehouse and data mart.'),\n",
       " Document(metadata={}, page_content='Developed YAML files for CI/CD pipelines to automate deployment processes, ensuring consistent code integration and efficient delivery across environments.'),\n",
       " Document(metadata={}, page_content='Deployed applications and data pipelines in Kubernetes clusters for development and production environments, leveraging Docker for containerization to enhance scalability and manageability.'),\n",
       " Document(metadata={}, page_content='Created personalized dashboard plugins where users could input reasons for machine states, utilizing Pareto and pie charts to analyze and visualize root causes of machine downtime.'),\n",
       " Document(metadata={}, page_content='Collaborated with software engineering teams to optimize data pipelines using PySpark for large-scale distributed data processing, significantly improving data processing speed and resource utilization.'),\n",
       " Document(metadata={}, page_content='Optimized the backend of a Grafana plugin using Golang and improved the frontend using TypeScript and React.'),\n",
       " Document(metadata={}, page_content='Company: Technische Universität Berlin, Role: Data Scientist'),\n",
       " Document(metadata={}, page_content='Utilized machine learning and deep learning techniques, including Random Forest, KNN, CNN, and Generative AI models, to predict key aircraft parameters such as range and flight mechanics.'),\n",
       " Document(metadata={}, page_content='Developed a Transformer-based behavior sequence model for a movie recommendation system, training the model on a dataset of 5 million entries and enhancing user personalization.'),\n",
       " Document(metadata={}, page_content='Conducted financial news sentiment analysis, classifying articles into positive, neutral, or negative sentiments using Random Forest and deep learning models.'),\n",
       " Document(metadata={}, page_content='Implemented time series forecasting models using LSTM, ARIMA, and Facebook Prophet for stock price prediction, comparing performance and supporting investment decisions.'),\n",
       " Document(metadata={}, page_content='Developed a deep learning-based customer segmentation model, achieving 98% accuracy for targeted marketing campaigns.'),\n",
       " Document(metadata={}, page_content='Utilized open-source large language models (LLMs) such as LLAMA 2.0 and Mixtral to extract and analyze contract information from legal documents, automating workflows.'),\n",
       " Document(metadata={}, page_content='Developed and deployed a chatbot system for Berlin’s public administration, utilizing open-source LLMs like ChatGPT 3.5, LLAMA, Alpaca, Vicuna 13b, Dolly 2.0, and Raven.'),\n",
       " Document(metadata={}, page_content='Built a Seq2Seq translation model with an attention mechanism for English-to-Bulgarian translation, using GloVe and fastText embeddings.'),\n",
       " Document(metadata={}, page_content='Developed a U-Net architecture for semantic segmentation of medical images to distinguish between cancerous and non-cancerous cells.'),\n",
       " Document(metadata={}, page_content='Built a multilabel image classification model using CNNs with Sklearn and PyTorch, applying preprocessing techniques with NumPy and Scipy for satellite image analysis.'),\n",
       " Document(metadata={}, page_content='Company: GetSteps, Role: Business Intelligence - Working Student'),\n",
       " Document(metadata={}, page_content='Designed and implemented data pipelines using PostgreSQL and dbt, improving data availability and streamlining ETL workflows.'),\n",
       " Document(metadata={}, page_content='Developed custom dashboards and KPI comparison reports in Metabase, providing insights into marketing performance and customer engagement.'),\n",
       " Document(metadata={}, page_content='Enhanced data marts by adding KPIs, enabling detailed cohort analysis for the marketing team.'),\n",
       " Document(metadata={}, page_content='Established a new data model for KPIs, improving transparency and making key metrics accessible for stakeholders.'),\n",
       " Document(metadata={}, page_content='Developed a new organizational structure for data marts to improve data governance.'),\n",
       " Document(metadata={}, page_content='Conducted custom cohort analysis for the marketing team using Jupyter Notebooks, examining Customer Lifetime Value (CLV).'),\n",
       " Document(metadata={}, page_content='Created a comprehensive shopping funnel report with KPIs and visualizations.'),\n",
       " Document(metadata={}, page_content='Generated an overview report for outstanding orders with discontinued cover materials, assisting the production team in managing inventory.'),\n",
       " Document(metadata={}, page_content='Conducted ad-hoc analyses for the CEO, including sales performance reports segmented by B2B and B2B2C.'),\n",
       " Document(metadata={}, page_content='Leveraged Google Analytics and Airbyte to import data from multiple sources, enhancing data comprehensiveness.'),\n",
       " Document(metadata={}, page_content='Company: Europcar Mobility Group, Role: Business Intelligence - Working Student'),\n",
       " Document(metadata={}, page_content='Designed reports in SAP Business Objects (BO) and Google Data Studio, providing actionable insights for decision-making.'),\n",
       " Document(metadata={}, page_content='Implemented ETL pipelines in BigQuery using SQL to extract and integrate data from SAP BO and Oracle.'),\n",
       " Document(metadata={}, page_content='Built dashboards in Looker to aid stakeholders across departments in tracking key metrics.'),\n",
       " Document(metadata={}, page_content='Automated reporting processes using VBA macros in Excel and Access, reducing manual effort.'),\n",
       " Document(metadata={}, page_content='Scheduled and managed automated report generation in SAP BO and virtual machines.'),\n",
       " Document(metadata={}, page_content='Developed detailed documentation for data marts and dashboards, supporting data governance initiatives.'),\n",
       " Document(metadata={}, page_content='Maintained and updated the company’s data model, ensuring consistency across multiple sources.'),\n",
       " Document(metadata={}, page_content='Conducted training sessions for business users on dashboard usage.'),\n",
       " Document(metadata={}, page_content='Company: Rohde & Schwarz, Role: BI Software Developer'),\n",
       " Document(metadata={}, page_content='Performed ETL processes to maintain a centralized data warehouse, enhancing data accessibility.'),\n",
       " Document(metadata={}, page_content='Developed time series forecasting models in Tableau and Python for sales predictions and inventory management.'),\n",
       " Document(metadata={}, page_content='Created interactive dashboards in Tableau to analyze sales performance and provide actionable insights.'),\n",
       " Document(metadata={}, page_content='Presented data-driven insights and visualizations to stakeholders.'),\n",
       " Document(metadata={}, page_content='Designed and maintained ETL workflows using SQL to ensure data consistency.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp_embedding, exp_texts, exp_metadata = generate_embedding(work_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syedalimuradtahir/Documents/Personal Projects/Portfolio_new/backend/Modules/Services/vector_store.py:34: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=embed_model, show_progress=True)\n",
      "/Users/syedalimuradtahir/Documents/Personal Projects/Portfolio_new/portfolio_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/syedalimuradtahir/Documents/Personal Projects/Portfolio_new/backend/Modules/Services/vector_store.py:42: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.51it/s]\n",
      "/Users/syedalimuradtahir/Documents/Personal Projects/Portfolio_new/backend/Modules/Services/vector_store.py:49: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "experience_store = store_embeddings(work_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syedalimuradtahir/Documents/Personal Projects/Portfolio_new/backend/Modules/Services/retriever.py:35: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=embed_model, show_progress=True)\n",
      "/Users/syedalimuradtahir/Documents/Personal Projects/Portfolio_new/portfolio_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/syedalimuradtahir/Documents/Personal Projects/Portfolio_new/backend/Modules/Services/retriever.py:39: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_db = Chroma(\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: Tesla, Role: Data Engineer\n",
      "Created engineering dashboards used by 100+ stakeholders across the USA, Germany, and Canada.\n",
      "Company: Technische Universität Berlin, Role: Data Scientist\n",
      "Developed and deployed a chatbot system for Berlin’s public administration, utilizing open-source LLMs like ChatGPT 3.5, LLAMA, Alpaca, Vicuna 13b, Dolly 2.0, and Raven.\n",
      "Optimized the backend of a Grafana plugin using Golang and improved the frontend using TypeScript and React.\n",
      "Utilized open-source large language models (LLMs) such as LLAMA 2.0 and Mixtral to extract and analyze contract information from legal documents, automating workflows.\n",
      "Implemented data visualizations to track machine states, allowing management to address downtime root causes and improve productivity.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What softwares did he used at Tesla. Did he use Python, SQL, Grafana, and Docker?\"\n",
    "retrieved_docs = retrieve_text('backend/Modules/vector_store/chromadb', query)\n",
    "retrieved_text = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "print(retrieved_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syedalimuradtahir/Documents/Personal Projects/Portfolio_new/backend/Modules/Services/llm_inference.py:35: LangChainDeprecationWarning: The class `HuggingFaceEndpoint` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  return HuggingFaceEndpoint(\n",
      "/Users/syedalimuradtahir/Documents/Personal Projects/Portfolio_new/portfolio_env/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Programming Languages: None mentioned for Tesla\n",
      "             - Software Names: None mentioned for Tesla\n"
     ]
    }
   ],
   "source": [
    "llm_model = Chatbot(FALCON_MODEL)\n",
    "response = llm_model.generate_text(query, context=retrieved_text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
