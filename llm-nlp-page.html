<html>
	<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Burny's Protfolio</title>


	<link href="https://fonts.googleapis.com/css?family=Space+Mono" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Kaushan+Script" rel="stylesheet">
	
	<!-- Animate.css -->
	<link rel="stylesheet" href="css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="css/bootstrap.css">

	<!-- Theme style  -->
	<link rel="stylesheet" href="css/style.css">

	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>
	<body>
		
	<div class="fh5co-loader"></div>

	<div id="fh5co-about" class="animate-box">
		<div class="container">
			<div class="row">
				<div class="col-md-8 col-md-offset-2 text-center fh5co-heading">
					<h2>But Do You Have Experience in NLP? A Question That Sparked a Journey</h2>
				</div>
			</div>
			<div class="row">
				<div class="col-md-12 text-center">
                    <p class="full-width-paragraph">
                    It was sometime between April and May of 2023 when a colleague, a seasoned data scientist, casually asked, 
                    "But do you have experience in Natural Language Processing?" I’d built machine learning models, tackled various data 
                    challenges, but NLP? That was uncharted territory. Intrigued by his work and inspired to expand my skillset, I decided 
                    to dive into the world of NLP and Large Language Models (LLMs). That semester, I committed myself fully, setting out to 
                    unravel the concept behind "Attention is All You Need" (no, not the song by Charlie Puth!). But first, I needed to grasp 
                    the magic of “Attention” itself—and before that, figure out how to actually *talk* to machines.
                    </p>
                    
                    <p class="full-width-paragraph">
                     To truly master NLP, I realized I’d need to dig into the fundamentals—not just the technical bits, but the practical, hands-on side of it,
                     too. What started as simple curiosity turned into a full-blown journey, sparking project after project: from extracting insights from complex 
                     contracts and crafting smarter recommendation systems, to building models that could assist public services and even translate languages. 
                     Let’s hope I’ve grabbed your *attention* by now! Below are some of my projects in NLP and LLM.
                    </p>
			</div>
		</div>
	</div>
	
    <div id="fh5co-work" class="fh5co-bg-dark">
		<div class="container">
			<div class="row animate-box">
				<div class="col-md-8 col-md-offset-2 text-center fh5co-heading">
					<h2>The Clearlea Challege</h2>
					<p>Since no one is working on the technical side, let’s actually develop the model,” I said… forgetting we had only 8 hours!</p>
				</div>
			</div>
		</div>
	</div>
    <div id="fh5co-about" class="animate-box">
        <div class="container">
            <div class="row">
                <div class="col-md-12 text-center">
                    <p class="full-width-paragraph">
                        One of my university marketing courses took us to a hackathon where we faced a unique challenge from a company called Clearlea. 
                        The company's mission was straightforward yet impactful: to take users' contract documents, analyze them, and provide clear answers 
                        on a user-friendly dashboard. Our task? To improve the user experience for their app. The founder was clearly hoping 
                        for some technical insights (or at least that’s what I thought). But with a course full of non-technical students, it seemed 
                        unlikely that he’d get what he was looking for. Meanwhile, I couldn’t help but think, “If I do this well, 
                        it’ll look fantastic on my GitHub!” Driven by that thought, I dived into the technical side, even when the 
                        founder remarked, “Wow, you look like one of the technical ones.” While others pitched general app improvements, 
                        I presented a different view: sometimes, customers don’t know exactly what they want. As Henry Ford famously said, 
                        “If I had asked people what they wanted, they would have said faster horses.”
                    </p>
                    <p class="full-width-paragraph">
                        And so, we got to work developing a model for Clearlea. Despite only having 8 hours, we managed to create a functional 
                        prototype. Our team tackled challenges like sourcing a relevant dataset for contract law—a task easier said than done. 
                        Without readily available data, we identified common parameters in contract laws (like start date, rent, and interest rate) 
                        and generated our own dataset using ChatGPT. For model selection, we initially considered LLaMA and Gemini but faced regional limitations. 
                        Ultimately, we utilized LLaMA3 70b for reliable insights and Mixtral 8x7b, a model Clearlea was already using. Our pipeline handled document
                        processing, question answering, and JSON-based result storage, enabling us to assess the model's performance. To evaluate output quality, we 
                        compared answers from both models against ChatGPT benchmarks and used manual scoring to validate accuracy. This collaborative effort with Clearlea’s team 
                        allowed us to deliver a refined, prototype solution within our tight timeline, offering meaningful enhancements to the user experience.
                        Our pipeline was designed to handle document processing, question answering, and results storage. I focused on setting up APIs and automating 
                        the pipeline to ensure seamless document analysis and result storage in JSON format, which allowed us to review and improve the model’s accuracy. 
                        To evaluate the models, we manually assessed their responses using ChatGPT as a benchmark, scoring each answer to refine our approach.
                    </p>
                    <p class="full-width-paragraph">
                        In the end, our team’s collaboration, along with insights from Clearlea’s partners, helped us overcome technical challenges and produce a working prototype in record time. It was an experience that strengthened my technical skills and highlighted the importance of teamwork and problem-solving.
                    </p>
                    <a href="https://github.com/stahir01/clearlea_challenge" class="btn btn-primary" target="_blank">
                        View on GitHub
                    </a>
                </div>
            </div>
        </div>
    </div>

    <div id="fh5co-work" class="fh5co-bg-dark">
		<div class="container">
			<div class="row animate-box">
				<div class="col-md-8 col-md-offset-2 text-center fh5co-heading">
					<h2>Chatbot Bobbi</h2>
                    <p> Let’s Make This Chatbot Great Again!</p>
				</div>
			</div>
		</div>
	</div>
    <div id="fh5co-about" class="animate-box">
        <div class="container">
            <div class="row">
                <div class="col-md-12 text-center">
                    <p class="full-width-paragraph">
                        The City of Berlin contains a Chatbot Bobbi to assist users in finding information about city services. However, due to 
                        its limitations in answering complex and diverse questions, our team set out to enhance its capabilities using Large Language Models (LLMs). 
                        By integrating models such as Chatgpt 3.5, LLaMA, Zicklein, and RWKV alongside a backend database, we aimed to build a chatbot that could provide more accurate, 
                        context-sensitive responses. Our architecture, as shown in the image below, includes a user interface built with Next.js, a FastAPI server, and an Apache SolR knowledge base 
                        that stores reliable information about Berlin’s services. When a user submits a query, the system checks the knowledge base for relevant information and uses a prompt 
                        generator to formulate questions for the LLMs. The responses from the selected LLMs are returned to the user, who can provide feedback through a star rating stored in an SQLite database. 
                        This feedback loop helps us understand the effectiveness of each model.
                    </p>
                    <div class="image-container">
                        <img src="images/chatbot_architecture.png" alt="Chatbot Bobbi System Architecture"  style="width: 50%; height: auto;">
                        <p><em>Figure: System Architecture of the Enhanced Chatbot Bobbi</em></p>
                    </div><br>
                    <p class="full-width-paragraph">
                    An example of a conversation is shown below. Some challenges we faced included handling hallucinations (when models provide plausible but 
                    incorrect answers) and ensuring quick response times, particularly for the locally deployed models. Despite these, our approach has shown promising results,
                     with improved response accuracy and user satisfaction compared to the original Chatbot Bobbi.</p><br>
                     <div class="image-container" style="width: 80%; margin: auto;">
                        <img src="images/chatexample.png" alt="Chat Example" style="width: 50%; height: auto;">
                        <p><em>Figure: Example of Enhanced Chatbot Bobbi Interaction</em></p>
                    </p>
                    </p>
                    <div class="button-container">
                        <a href="https://github.com/stahir01/chatbot-berlin" class="btn btn-primary" target="_blank">
                            View on GitHub
                        </a>
                        <a href="https://ceur-ws.org/Vol-3630/LWDA2023-paper27.pdf" class="btn btn-primary" target="_blank">
                            View Research Paper
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </div><br>
	
    <div id="fh5co-work" class="fh5co-bg-dark">
		<div class="container">
			<div class="row animate-box">
				<div class="col-md-8 col-md-offset-2 text-center fh5co-heading">
					<h2>Transform Your Watchlist: A Smart, Sequence-Savvy Recommender</h2>
                    <p> Your Next Favorite Movie Isn’t Random – It’s Science!</p>
				</div>
			</div>
		</div>
	</div>
    <div id="fh5co-about" class="animate-box">
        <div class="container">
            <div class="row">
                <div class="col-md-12 text-center">
                    <p class="full-width-paragraph">
                        Our project takes recommendation systems to the next level by using transformers to make smart, personalized recommendations based on your unique viewing history. 
                        Traditional recommendation systems often miss the importance of interaction order, so they don’t capture your changing tastes over time. This is where our <strong>Behavior Sequence Transformer</strong> 
                        steps in, analyzing your past choices in sequence to serve up recommendations that actually make sense!
                    </p>
                    <p class="full-width-paragraph">
                        Our approach involved feeding user interaction data into a transformer model, where each item's position in the sequence was encoded to reflect temporal relevance. 
                        We trained the model using popular datasets, such as the MovieLens 1M and 25M datasets, which allowed us to refine recommendations based on detailed movie ratings and 
                        viewing patterns. The transformer architecture’s self-attention mechanism enabled the model to identify intricate patterns in user behavior, ultimately generating highly personalized 
                        and relevant recommendations.
                    </p>
                    <p class="full-width-paragraph">

                    </p>
                    <a href="https://github.com/stahir01/Neuronale_Informationsverarbeitung" class="btn btn-primary" target="_blank">
                        View on GitHub
                    </a>
                </div>
            </div>
        </div>
    </div>

    <div id="fh5co-work" class="fh5co-bg-dark">
		<div class="container">
			<div class="row animate-box">
				<div class="col-md-8 col-md-offset-2 text-center fh5co-heading">
					<h2>From Cyrillic to English with a Neural Spin!</h2>
                    <p> Building a Translator that Understands the Story Behind Every Sentence</p>
				</div>
			</div>
		</div>
	</div>
    <div id="fh5co-about" class="animate-box">
        <div class="container">
            <div class="row">
                <div class="col-md-12 text-center">
                    <p>
                    The goal of this project was to develop a neural machine translation model capable of translating between English and Bulgarian.
                    Using the European Parliament Proceedings Parallel Corpus, we trained our models on both languages, focusing on capturing meaningful 
                    translations and handling complex language structures. To keep the training efficient, we utilized Google Colab and focused on half of the dataset,
                    balancing performance with computational limits.
                    </p>

                    <p>
                    We experimented with various model architectures, including standard Recurrent Neural Networks (RNNs) and an improved attention-based RNN model. 
                    The RNN model used **GloVe** embeddings for English and **FastText** for Bulgarian, transforming words into dense vectors that capture semantic relationships. 
                    By incorporating attention mechanisms, we achieved a substantial improvement in translation accuracy, especially on longer sentences where the attention model could
                    focus on specific parts of the sequence.
                    </p>
        
                    <p>
                    The final results showed that the attention model outperformed standard RNNs, delivering significantly higher BLEU and METEOR scores. However, due to memory constraints, 
                    we evaluated the model on a limited sample, but it showed promising results for future expansion. This project highlighted the power of attention in neural machine translation, 
                    allowing our model to translate effectively between Bulgarian and English.
                    </p>

                    <a href="https://github.com/stahir01/Machine-Translation" class="btn btn-primary" target="_blank">
                        View on GitHub
                    </a>
                </div>
            </div>
        </div>
    </div>



    <div id="fh5co-work" class="fh5co-bg-dark">
		<div class="container">
			<div class="row animate-box">
				<div class="col-md-8 col-md-offset-2 text-center fh5co-heading">
					<h2>Decoding Financial Sentiment: A Deep Dive into Market Mood</h2>
                    <p> Turning Headlines into Insights with NLP-Powered Sentiment Analysis</p>
				</div>
			</div>
		</div>
	</div>
    <div id="fh5co-about" class="animate-box">
        <div class="container">
            <div class="row">
                <div class="col-md-12 text-center">
                    <p>This project aimed to analyze the sentiment of financial news headlines, transforming them into valuable insights for market trends. Using a dataset of financial sentences labeled as positive, neutral, or negative, we applied a range of pre-processing techniques, including lower-casing, punctuation removal, and tokenization. This helped create a clean input for our models, allowing them to focus on the language patterns that indicate sentiment.</p>

                    <p>We tested various machine learning models, such as **Naïve Bayes**, **Feed-Forward Neural Networks (FFN)**, and **Random Forests**. For feature extraction, we experimented with Count Vectorization and TF-IDF to understand which method best captured the nuances of financial language. Imbalanced data presented a challenge, so we used oversampling techniques to improve the performance, especially for underrepresented classes.</p>
        
                    <p>Our results showed that the **Random Forest model** performed best on the balanced dataset, achieving a high F1-score of 0.91, indicating its superior ability to capture sentiment. This project illustrates how NLP techniques can unlock valuable insights from financial text, helping interpret market sentiment more accurately.</p>
                    
                    <a href="https://github.com/stahir01/Sentiment-Analysis-Financial-News" class="btn btn-primary" target="_blank">
                        View on GitHub
                    </a>
                </div>
            </div>
        </div>
    </div>


	
	
	<!-- jQuery -->
	<script src="js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script src="js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script src="js/jquery.waypoints.min.js"></script>
	<!-- Stellar Parallax -->
	<script src="js/jquery.stellar.min.js"></script>
	<!-- Easy PieChart -->
	<script src="js/jquery.easypiechart.min.js"></script>
	<!-- Google Map -->
	<script src="https://maps.googleapis.com/maps/api/js?key=AIzaSyCefOgb1ZWqYtj7raVSmN4PL2WkTrc-KyA&sensor=false"></script>
	<script src="js/google_map.js"></script>
	
	<!-- Main -->
	<script src="js/main.js"></script>

	</body>
</html>

