<html>
	<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>NLP & LLM Experience</title>


	<link href="https://fonts.googleapis.com/css?family=Space+Mono" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Kaushan+Script" rel="stylesheet">
	
	<!-- Animate.css -->
	<link rel="stylesheet" href="css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="css/bootstrap.css">

	<!-- Theme style  -->
	<link rel="stylesheet" href="css/style.css">

	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>
	<body>
		
	<div class="fh5co-loader"></div>

	<div id="fh5co-about" class="animate-box">
		<div class="container">
			<div class="row">
				<div class="col-md-8 col-md-offset-2 text-center fh5co-heading">
					<h2>But Do You Have Experience in NLP? A Question That Sparked a Journey</h2>
				</div>
			</div>
			<div class="row">
				<div class="col-md-12 text-center">
                    <p class="full-width-paragraph">
                    It was sometime between April and May of 2023 when a colleague, a seasoned data scientist, casually asked, 
                    "But do you have experience in Natural Language Processing?" I’d built machine learning models, tackled various data 
                    challenges, but NLP? That was uncharted territory. Intrigued by his work and inspired to expand my skillset, I decided 
                    to dive into the world of NLP and Large Language Models (LLMs). That semester, I committed myself fully, setting out to 
                    unravel the concept behind "Attention is All You Need" (no, not the song by Charlie Puth!). But first, I needed to grasp 
                    the magic of “Attention” itself—and before that, figure out how to actually *talk* to machines.
                    </p>
                    
                    <p class="full-width-paragraph">
                     To truly master NLP, I realized I’d need to dig into the fundamentals—not just the technical bits, but the practical, hands-on side of it,
                     too. What started as simple curiosity turned into a full-blown journey, sparking project after project: from extracting insights from complex 
                     contracts and crafting smarter recommendation systems, to building models that could assist public services and even translate languages. 
                     Let’s hope I’ve grabbed your *attention* by now! Below are some of my projects in NLP and LLM.
                    </p>
			</div>
		</div>
	</div>
	
    <div id="fh5co-work" class="fh5co-bg-dark">
		<div class="container">
			<div class="row animate-box">
				<div class="col-md-8 col-md-offset-2 text-center fh5co-heading">
					<h2>The Clearlea Challege</h2>
					<p>Since no one is working on the technical side, let’s actually develop the model,” I said… forgetting we had only 8 hours!</p>
				</div>
			</div>
		</div>
	</div>
    <div id="fh5co-about" class="animate-box">
        <div class="container">
            <div class="row">
                <div class="col-md-12 text-center">
                    <p class="full-width-paragraph">
                        One of my university marketing courses took us to a hackathon where we faced a unique challenge from a company called Clearlea. 
                        The company's mission was straightforward yet impactful: to take users' contract documents, analyze them, and provide clear answers 
                        on a user-friendly dashboard. Our task? To improve the user experience for their app. The founder was clearly hoping 
                        for some technical insights (or at least that’s what I thought). But with a course full of non-technical students, it seemed 
                        unlikely that he’d get what he was looking for. Meanwhile, I couldn’t help but think, “If I do this well, 
                        it’ll look fantastic on my GitHub!” Driven by that thought, I dived into the technical side, even when the 
                        founder remarked, “Wow, you look like one of the technical ones.” While others pitched general app improvements, 
                        I presented a different view: sometimes, customers don’t know exactly what they want. As Henry Ford famously said, 
                        “If I had asked people what they wanted, they would have said faster horses.”
                    </p>
                    <p class="full-width-paragraph">
                        And so, we got to work developing a model for Clearlea. Despite only having 8 hours, we managed to create a functional 
                        prototype. Our team tackled challenges like sourcing a relevant dataset for contract law—a task easier said than done. 
                        Without readily available data, we identified common parameters in contract laws (like start date, rent, and interest rate) 
                        and generated our own dataset using ChatGPT. For model selection, we initially considered LLaMA and Gemini but faced regional limitations. 
                        Ultimately, we utilized LLaMA3 70b for reliable insights and Mixtral 8x7b, a model Clearlea was already using. Our pipeline handled document
                        processing, question answering, and JSON-based result storage, enabling us to assess the model's performance. To evaluate output quality, we 
                        compared answers from both models against ChatGPT benchmarks and used manual scoring to validate accuracy. This collaborative effort with Clearlea’s team 
                        allowed us to deliver a refined, prototype solution within our tight timeline, offering meaningful enhancements to the user experience.
                        Our pipeline was designed to handle document processing, question answering, and results storage. I focused on setting up APIs and automating 
                        the pipeline to ensure seamless document analysis and result storage in JSON format, which allowed us to review and improve the model’s accuracy. 
                        To evaluate the models, we manually assessed their responses using ChatGPT as a benchmark, scoring each answer to refine our approach.
                    </p>
                    <div class="image-container">
                        <img src="images/ClearleaArchitecture1.png" alt="ClearLea Architecture - Part 1" style="width: 50%; height: auto;">
                        <img src="images/ClearleaArchitecture2.png" alt="ClearLea Architecture - Part 2" style="width: 50%; height: auto;">
                        <p><em>Figure: Data Pipeline & Model Architecture.</em></p>
                    </div>
                    <p class="full-width-paragraph">
                        In the end, our team’s collaboration, along with insights from Clearlea’s partners, helped us overcome technical challenges and produce a working prototype in record time. It was an experience that strengthened my technical skills and highlighted the importance of teamwork and problem-solving.
                    </p>
                    <a href="https://github.com/stahir01/clearlea_challenge" class="btn btn-primary" target="_blank">
                        View on GitHub
                    </a>
                </div>
            </div>
        </div>
    </div>

    <div id="fh5co-work" class="fh5co-bg-dark">
		<div class="container">
			<div class="row animate-box">
				<div class="col-md-8 col-md-offset-2 text-center fh5co-heading">
					<h2>Chatbot Bobbi</h2>
                    <p> Let’s Make This Chatbot Great Again!</p>
				</div>
			</div>
		</div>
	</div>
    <div id="fh5co-about" class="animate-box">
        <div class="container">
            <div class="row">
                <div class="col-md-12 text-center">
                    <p class="full-width-paragraph">
                        The City of Berlin contains a Chatbot Bobbi to assist users in finding information about city services. However, due to 
                        its limitations in answering complex and diverse questions, our team set out to enhance its capabilities using Large Language Models (LLMs). 
                        By integrating models such as Chatgpt 3.5, LLaMA, Zicklein, and RWKV alongside a backend database, we aimed to build a chatbot that could provide more accurate, 
                        context-sensitive responses. Our architecture, as shown in the image below, includes a user interface built with Next.js, a FastAPI server, and an Apache SolR knowledge base 
                        that stores reliable information about Berlin’s services. When a user submits a query, the system checks the knowledge base for relevant information and uses a prompt 
                        generator to formulate questions for the LLMs. The responses from the selected LLMs are returned to the user, who can provide feedback through a star rating stored in an SQLite database. 
                        This feedback loop helps us understand the effectiveness of each model.
                    </p>
                    <div class="image-container">
                        <img src="images/chatbot_architecture.png" alt="Chatbot Bobbi System Architecture"  style="width: 50%; height: auto;">
                        <p><em>Figure: System Architecture of the Enhanced Chatbot Bobbi</em></p>
                    </div><br>
                    <p class="full-width-paragraph">
                    An example of a conversation is shown below. Some challenges we faced included handling hallucinations (when models provide plausible but 
                    incorrect answers) and ensuring quick response times, particularly for the locally deployed models. Despite these, our approach has shown promising results,
                     with improved response accuracy and user satisfaction compared to the original Chatbot Bobbi.</p><br>
                     <div class="image-container" style="width: 80%; margin: auto;">
                        <img src="images/chatexample.png" alt="Chat Example" style="width: 50%; height: auto;">
                        <p><em>Figure: Example of Enhanced Chatbot Bobbi Interaction</em></p>
                    </p>
                    </p>
                    <div class="button-container">
                        <a href="https://github.com/stahir01/chatbot-berlin" class="btn btn-primary" target="_blank">
                            View on GitHub
                        </a>
                        <a href="https://ceur-ws.org/Vol-3630/LWDA2023-paper27.pdf" class="btn btn-primary" target="_blank">
                            View Research Paper
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </div><br>
	
    <div id="fh5co-work" class="fh5co-bg-dark">
        <div class="container">
            <div class="row animate-box">
                <div class="col-md-8 col-md-offset-2 text-center fh5co-heading">
                    <h2>Transform Your Watchlist: A Smart, Sequence-Savvy Recommender</h2>
                    <p>Your Next Favorite Movie Isn’t Random – It’s Science!</p>
                </div>
            </div>
        </div>
    </div>
    
    <div id="fh5co-about" class="animate-box">
        <div class="container">
            <div class="row">
                <div class="col-md-12 text-center">
                    <p class="full-width-paragraph">
                        Every movie you watch, every title you skip, and every rating you give tells a story – not just about what you like, 
                        but how your preferences evolve over time. Traditional recommendation systems often ignore this sequence, treating 
                        your history like a static checklist. But preferences are dynamic, like a moving painting, constantly reshaping.
                    </p>
                    <p class="full-width-paragraph">
                        Enter the <strong>Behavior Sequence Transformer</strong>, a system that listens to the rhythm of your choices. It doesn’t just suggest movies; it weaves together 
                        patterns, detects shifts in interests, and evolves with you. Using transformers – the backbone of modern AI – we created a system that sees beyond isolated data points. 
                        It captures the <em>context</em>, the <em>order</em>, and the <em>why</em> behind your preferences.
                    </p>
    
                    <!-- Architecture Image -->
                    <div class="image-container">
                        <img src="images/Behaviour sequence transformer.png" alt="Behavior Sequence Transformer Architecture" style="width: 50%; height: auto; border-radius: 50%;">
                        <p><em>Architecture of the Behavior Sequence Transformer.</em></p>
                    </div>
    
                    <p class="full-width-paragraph">
                        The architecture combines contextual user data (like movie genres, ratings, and user profiles) with the self-attention mechanisms of transformers. 
                        Every sequence is a story, and every attention layer learns its meaning. By understanding these interactions, the system ensures that your next 
                        recommendation feels less like a guess and more like a perfectly-timed suggestion.
                    </p>
    
                    <!-- Results Section in a Box -->
                    <div class="results-box" style="border: 1px solid #ccc; padding: 20px; margin-top: 30px; border-radius: 10px; background-color: #f9f9f9;">
                        <h3 style="text-align: center; margin-bottom: 20px;">Results</h3>
                        <div class="row">
                            <!-- First Row of Images -->
                            <div class="col-md-6" style="float: left; width: 50%; padding: 10px;">
                                <img src="images/BST_sequence_length_plot.png" alt="Model Performance for Sequence Lengths" style="width: 100%; height: auto; border-radius: 10px;">
                                <p><em>Impact of sequence length</em></p>
                            </div>
                            <div class="col-md-6" style="float: left; width: 50%; padding: 10px;">
                                <img src="images/BST_test_rmse_score_for_epochs.png" alt="Test RMSE Across Epochs" style="width: 100%; height: auto; border-radius: 10px;">
                            </div>
                        </div>
                        <!-- Second Row of Images -->
                        <div class="row" style="clear: both; margin-top: 20px;">
                            <div class="col-md-12" style="padding: 10px;">
                                <img src="images/BST_Transformer model vs Traditional recommendation systems].png" alt="Comparison of Transformer and Traditional Systems" style="width: 50%; height: auto; border-radius: 10px;">
                            </div>
                        </div>
                    </div>
    
                    <p class="full-width-paragraph">
                        These results highlight the transformational power of our model. As seen in the sequence length analysis, the system carefully balances 
                        its ability to learn from longer histories without overfitting. By optimizing the number of training epochs, we ensured the model reached 
                        peak performance while maintaining computational efficiency. The comparison with traditional systems showcases the significance of sequence-aware 
                        modeling, solidifying our approach as a game-changer in recommendation systems.
                    </p>
                    <p class="full-width-paragraph">
                        Imagine a system that doesn’t just predict what you might want to watch, but grows alongside you, adapting to your ever-changing tastes.                     </p>
                    <a href="https://github.com/stahir01/Neuronale_Informationsverarbeitung" class="btn btn-primary" target="_blank">
                        View on GitHub
                    </a>
                </div>
            </div>
        </div>
    </div>
    

    <div id="fh5co-work" class="fh5co-bg-dark">
        <div class="container">
            <div class="row animate-box">
                <div class="col-md-8 col-md-offset-2 text-center fh5co-heading">
                    <h2>From Cyrillic to English with a Neural Spin!</h2>
                    <p>Building a Translator that Understands the Story Behind Every Sentence</p>
                </div>
            </div>
        </div>
    </div>
    
    <div id="fh5co-about" class="animate-box">
        <div class="container">
            <div class="row">
                <div class="col-md-12 text-center">
                    <p>
                        Translating languages isn’t just about replacing words—it’s about understanding their essence. Using 
                        the European Parliament Proceedings Parallel Corpus, we trained a neural machine translation model capable of translating between English 
                        and Bulgarian. This task presented unique challenges, from Cyrillic alphabet handling to capturing complex syntax. Here’s how we tackled it.
                    </p>
                    <p>
                        Preprocessing played a pivotal role in preparing the dataset for neural learning. We tokenized sentences, normalized case, 
                        and leveraged pre-trained word embeddings—<strong>GloVe</strong> for English and <strong>FastText</strong> for Bulgarian. These embeddings 
                        transformed words into dense semantic vectors, capturing both meaning and context.
                    </p>
                    <p>
                        We experimented with various model architectures, including standard Recurrent Neural Networks (RNNs) and an improved attention-based RNN model. 
                        The RNN model used <strong>GloVe</strong> embeddings for English and <strong>FastText</strong> for Bulgarian, transforming words into dense 
                        vectors that capture semantic relationships. By incorporating attention mechanisms, we achieved a substantial improvement in translation accuracy, 
                        especially on longer sentences where the attention model could focus on specific parts of the sequence.
                    </p>
    
                    <!-- Model Architectures Section -->
                    <div class="results-box" style="border: 1px solid #ccc; padding: 20px; margin-top: 30px; border-radius: 10px; background-color: #f9f9f9;">
                        <h3 style="text-align: center; margin-bottom: 20px;">Model Architectures</h3>
                        <div class="row">
                            <!-- RNN-Based Seq2Seq Model -->
                            <div class="col-md-6 text-center">
                                <div style="border: 1px solid #ddd; padding: 15px; border-radius: 10px; background-color: #f0f0f0;">
                                    <h4 style="color: #2c3e50;">RNN-Based Seq2Seq Model</h4>
                                    <pre style="text-align: left; font-size: 11px; overflow-x: auto; background: #272822; color: #f8f8f2; padding: 10px; border-radius: 8px;">
    
    def seq2seq_model(input_vocab_size, output_vocab_size, encoder_embedding_matrix, decoder_embedding_matrix, 
                      embedding_dim=300, max_length=25, hidden_units=128, change_targets=False):
        if not change_targets:  # English to Bulgarian
            encoder_inputs = Input(shape=(max_length,))
            encoder_embedding = Embedding(input_vocab_size, embedding_dim, weights=[encoder_embedding_matrix], 
                                           input_length=max_length, mask_zero=True)(encoder_inputs)
            encoder_lstm = LSTM(hidden_units)(encoder_embedding)
            decoder_inputs = Input(shape=(None,))
            decoder_embedding = Embedding(output_vocab_size, embedding_dim, weights=[decoder_embedding_matrix])(decoder_inputs)
            decoder_lstm = LSTM(hidden_units, return_sequences=True)(decoder_embedding)
            decoder_outputs = Dense(output_vocab_size, activation='softmax')(decoder_lstm)
            model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
        else:  # Bulgarian to English
            ...
        return model
                                    </pre>
                                </div>
                            </div>
                            
                            <!-- Attention-Based Seq2Seq Model -->
                            <div class="col-md-6 text-center">
                                <div style="border: 1px solid #ddd; padding: 15px; border-radius: 10px; background-color: #f0f0f0;">
                                    <h4 style="color: #2c3e50;">Attention-Based Seq2Seq Model</h4>
                                    <pre style="text-align: left; font-size: 11px; overflow-x: auto; background: #272822; color: #f8f8f2; padding: 10px; border-radius: 8px;">
    
    def seq2seq_attention(input_vocab_size, output_vocab_size, encoder_embedding_matrix, decoder_embedding_matrix, 
                          embedding_dim=300, max_length=25, hidden_units=128, dropout_rate=0.2):
        encoder_inputs = Input(shape=(max_length,))
        encoder_embedding = Embedding(input_vocab_size, embedding_dim, weights=[encoder_embedding_matrix], 
                                       input_length=max_length, mask_zero=True)(encoder_inputs)
        encoder_lstm = LSTM(hidden_units, return_sequences=True)(encoder_embedding)
        encoder_lstm = Dropout(dropout_rate)(encoder_lstm)
        decoder_inputs = Input(shape=(None,))
        decoder_embedding = Embedding(output_vocab_size, embedding_dim, weights=[decoder_embedding_matrix])(decoder_inputs)
        decoder_lstm = LSTM(hidden_units, return_sequences=True)(decoder_embedding)
        decoder_lstm = Dropout(dropout_rate)(decoder_lstm)
        cross_attention = CrossAttention(units=hidden_units)
        decoder_attention = cross_attention(decoder_lstm, encoder_lstm)
        decoder_outputs = Dense(output_vocab_size, activation='softmax')(decoder_attention)
        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
        return model
                                    </pre>
                                </div>
                            </div>
                        </div>
                    </div>
    
                    <p>
                        The final results showed that the attention model outperformed standard RNNs, delivering significantly higher BLEU and METEOR scores. 
                        However, due to memory constraints, we evaluated the model on a limited sample, but it showed promising results for future expansion. 
                        This project highlighted the power of attention in neural machine translation, allowing our model to translate effectively between Bulgarian and English.
                    </p>
                    <a href="https://github.com/stahir01/Machine-Translation" class="btn btn-primary" target="_blank">
                        View on GitHub
                    </a>
                </div>
            </div>
        </div>
    </div>
    


    <div id="fh5co-work" class="fh5co-bg-dark">
        <div class="container">
            <div class="row animate-box">
                <div class="col-md-8 col-md-offset-2 text-center fh5co-heading">
                    <h2>Decoding Financial Sentiment: A Deep Dive into Market Mood</h2>
                    <p>Turning Headlines into Insights with NLP-Powered Sentiment Analysis</p>
                </div>
            </div>
        </div>
    </div>
    
    <div id="fh5co-about" class="animate-box">
        <div class="container">
            <div class="row">
                <div class="col-md-12 text-center">
                    <p>
                        Markets are driven by emotion, and headlines are often their loudest echoes. This project aimed to unlock the sentiment 
                        hidden in financial news—transforming headlines into actionable insights that could help investors gauge market mood 
                        and make informed decisions.
                    </p>
                    <p>
                        Financial headlines aren’t just text; they’re a treasure trove of nuance, statistics, and implications. Our first challenge was taming this raw data, 
                        which came with its quirks—short sentences, symbol-heavy phrasing, and a noticeable imbalance in sentiment classes where neutral headlines dominated. 
                        To address these challenges, we implemented a sophisticated preprocessing pipeline that included steps like lowercasing, symbol replacement, lemmatization, 
                        and tokenization. These transformations turned noisy data into a structured format, laying the foundation for effective analysis.
                    </p>
                    <p>
                        To classify sentiment, we tested models including Naïve Bayes, Feed-Forward Neural Networks (FFN), and Random Forest. Among these, Random Forest 
                        emerged as the top performer, achieving an impressive F1-score of 0.91. Its ability to handle the complexities of balanced data highlighted its suitability 
                        for this task, while oversampling techniques significantly improved the performance of all models.
                    </p>
    
                    <!-- Image and Results Section -->
                    <div class="results-box" style="border: 1px solid #ccc; padding: 20px; margin-top: 30px; border-radius: 10px; background-color: #f9f9f9;">
                        <div class="row">
                            <!-- Naive Bayes Image -->
                            <div class="col-md-6 text-center" style="padding: 10px;">
                                <img src="images/FinancialSentimentalAnalysis_Class Imbalance Oversample Minority Class (naives_bayes_model.png" alt="Naïve Bayes Classifier Results" style="width: 70%; height: auto; border-radius: 10px;">
                                <p><em>Naïve Bayes classifier results with oversampled minority class.</em></p>
                            </div>
                            <!-- Random Forest Image -->
                            <div class="col-md-6 text-center" style="padding: 10px;">
                                <img src="images/FinancialSentimentalAnalysis_Class Imbalance Oversample Minority Class (RandomForest).png" alt="Random Forest Classifier Results" style="width: 70%; height: auto; border-radius: 10px;">
                                <p><em>Random Forest classifier results with oversampled minority class.</em></p>
                            </div>
                        </div>
                    </div>
                    <p>
                        This project demonstrated how NLP techniques can decode market sentiment, turning financial headlines into valuable insights. 
                        By applying the right preprocessing, feature engineering, and modeling strategies, we showcased the potential of data-driven 
                        decision-making in finance.
                    </p>
                    <a href="https://github.com/stahir01/Sentiment-Analysis-Financial-News" class="btn btn-primary" target="_blank">
                        View on GitHub
                    </a>
                </div>
            </div>
        </div>
    </div>

    
	
	<!-- jQuery -->
	<script src="js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script src="js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script src="js/jquery.waypoints.min.js"></script>
	<!-- Stellar Parallax -->
	<script src="js/jquery.stellar.min.js"></script>
	<!-- Easy PieChart -->
	<script src="js/jquery.easypiechart.min.js"></script>
	<!-- Google Map -->
	<script src="https://maps.googleapis.com/maps/api/js?key=AIzaSyCefOgb1ZWqYtj7raVSmN4PL2WkTrc-KyA&sensor=false"></script>
	<script src="js/google_map.js"></script>
	
	<!-- Main -->
	<script src="js/main.js"></script>

	</body>
</html>

